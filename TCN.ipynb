{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9313ec05-10ef-437c-999f-02b7bc762c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 26 16:20:11 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:4C:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              74W / 400W |   7262MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          Off | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              51W / 400W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1aa48f-406f-4037-98d6-81456d5b1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import copy\n",
    "from pylab import show\n",
    "from typing import Union, Optional, Callable\n",
    "from torch import Tensor\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import torchaudio\n",
    "from torchmetrics import ROC\n",
    "\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "import dgl\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import soundfile as sf\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import wandb\n",
    "import traceback\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from IPython.display import clear_output, Audio\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import scipy.signal as dsp\n",
    "from scipy import fft\n",
    "import torchaudio\n",
    "\n",
    "from dataset import Dataset_ASVspoof2019_train, Dataset_ASVspoof2019_devNeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03935c91-2fed-4df9-a799-2fe812c5c3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mslenser0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/borodin_sam/aasist/wandb/run-20240227_093249-82mvwo03</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/slenser0/AASIST/runs/82mvwo03' target=\"_blank\">twilight-silence-66</a></strong> to <a href='https://wandb.ai/slenser0/AASIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/slenser0/AASIST' target=\"_blank\">https://wandb.ai/slenser0/AASIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/slenser0/AASIST/runs/82mvwo03' target=\"_blank\">https://wandb.ai/slenser0/AASIST/runs/82mvwo03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(\n",
    "        project=\"AASIST\",\n",
    "        config={\n",
    "            \"epochs\": 70,\n",
    "            \"batch_size\": 28,\n",
    "            \"lr\": 0.0001,\n",
    "            })\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9a559a-4ee4-49fb-b3f4-829eb8ad862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(it, prefix=\"\", size=60, out=sys.stdout): # Python3.6+\n",
    "    count = len(it)\n",
    "    start = time.time()\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        remaining = ((time.time() - start) / j) * (count - j)\n",
    "        passing = time.time() - start\n",
    "        mins_pas, sec_pass = divmod(passing, 60)\n",
    "        time_pas = f\"{int(mins_pas):02}:{sec_pass:05.2f}\"\n",
    "        \n",
    "        \n",
    "        mins, sec = divmod(remaining, 60)\n",
    "        time_str = f\"{int(mins):02}:{sec:05.2f}\"\n",
    "        \n",
    "        \n",
    "        print(f\"{prefix}[{u'â–ˆ'*x}{('.'*(size-x))}] {j}/{count} time {time_pas} / {time_str}\", end='\\r', file=out, flush=True)\n",
    "        \n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i+1)\n",
    "    print(\"\\n\", flush=True, file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a516420-6b51-4faf-8c36-78a8a0fefe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 666666\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    dgl.seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35fecee3-279b-4fb7-b911-1dd9c38f3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SincConv_fast(nn.Module):\n",
    "    @staticmethod\n",
    "    def to_mel(hz):\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_hz(mel):\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\n",
    "\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\n",
    "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=0, min_band_hz=0):\n",
    "\n",
    "        super(SincConv_fast,self).__init__()\n",
    "\n",
    "        if in_channels != 1:\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        if kernel_size%2==0:\n",
    "            self.kernel_size=self.kernel_size+1\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if bias:\n",
    "            raise ValueError('SincConv does not support bias.')\n",
    "        if groups > 1:\n",
    "            raise ValueError('SincConv does not support groups.')\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_low_hz = min_low_hz\n",
    "        self.min_band_hz = min_band_hz\n",
    "\n",
    "        low_hz = 0\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
    "\n",
    "        mel = np.linspace(self.to_mel(low_hz),\n",
    "                          self.to_mel(high_hz),\n",
    "                          self.out_channels + 1)\n",
    "        hz = self.to_hz(mel)\n",
    "\n",
    "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
    "\n",
    "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
    "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2)))\n",
    "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n",
    "\n",
    "        n = (self.kernel_size - 1) / 2.0\n",
    "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "\n",
    "        self.n_ = self.n_.to(waveforms.device)\n",
    "\n",
    "\n",
    "        self.window_ = self.window_.to(waveforms.device)\n",
    "\n",
    "        low = self.min_low_hz  + torch.abs(self.low_hz_)\n",
    "\n",
    "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n",
    "        band=(high-low)[:,0]\n",
    "\n",
    "        f_times_t_low = torch.matmul(low, self.n_)\n",
    "        f_times_t_high = torch.matmul(high, self.n_)\n",
    "\n",
    "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_\n",
    "        band_pass_center = 2*band.view(-1,1)\n",
    "        band_pass_right= torch.flip(band_pass_left,dims=[1])\n",
    "\n",
    "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n",
    "\n",
    "\n",
    "        band_pass = band_pass / (2*band[:,None])\n",
    "\n",
    "        self.filters = (band_pass).view(\n",
    "            self.out_channels, 1, self.kernel_size)\n",
    "\n",
    "        return F.conv1d(waveforms, self.filters, stride=self.stride,\n",
    "                        padding=self.padding, dilation=self.dilation,\n",
    "                         bias=None, groups=1)\n",
    "\n",
    "\n",
    "\n",
    "class Res2Block(nn.Module):\n",
    "    def __init__(self, nb_filts, nums=4):\n",
    "        super(Res2Block, self).__init__()\n",
    "        self.nb_filts = nb_filts\n",
    "        self.conv1 = nn.Conv2d(in_channels=nb_filts[0],\n",
    "                               out_channels=nb_filts[1],\n",
    "                               kernel_size=1,\n",
    "                               padding=0,\n",
    "                               stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=nb_filts[1])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.nums = nums\n",
    "        self.SE = SE_Block(nb_filts[1])\n",
    "\n",
    "        convs = []\n",
    "        bns = []\n",
    "\n",
    "        for i in range(self.nums):\n",
    "            convs.append(nn.Conv2d(in_channels=(nb_filts[1]// self.nums),\n",
    "                                   out_channels=(nb_filts[1] //self.nums),\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1))\n",
    "            bns.append(nn.BatchNorm2d((nb_filts[1] //self.nums)))\n",
    "\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.bns = nn.ModuleList(bns)\n",
    "\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=nb_filts[1],\n",
    "                               out_channels=nb_filts[1],\n",
    "                               kernel_size=1,\n",
    "                               padding=0,\n",
    "                               stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(nb_filts[1])\n",
    "\n",
    "        if nb_filts[0] != nb_filts[1]:\n",
    "            self.downsample = True\n",
    "            self.conv_downsample = nn.Conv2d(in_channels=nb_filts[0],\n",
    "                                             out_channels=nb_filts[1],\n",
    "                                             padding=(0, 1),\n",
    "                                             kernel_size=(1, 3),\n",
    "                                             stride=1)\n",
    "        else:\n",
    "            self.downsample = False\n",
    "\n",
    "        self.mp = nn.MaxPool2d((1,3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        spx = torch.split(out, self.nb_filts[1]//self.nums, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i==0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp += spx[i]\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.bns[i](sp)\n",
    "\n",
    "            if i==0:\n",
    "                out = sp\n",
    "            else:\n",
    "                out = torch.cat((out,sp),1)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.SE(out)\n",
    "\n",
    "        if self.downsample:\n",
    "            residual = self.conv_downsample(residual)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.mp(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SE_Block(nn.Module):\n",
    "    \"credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4\"\n",
    "    def __init__(self, c, r=8):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(c, c // r, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(c // r, c, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.shape\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        y = self.excitation(y).view(bs, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        filts = [70, [1, 32], [32, 32], [32, 64], [64, 64]]\n",
    "\n",
    "        self.sinc_conv = SincConv_fast(out_channels=filts[0],\n",
    "                                  kernel_size=128,\n",
    "        )\n",
    "\n",
    "        self.first_bn = nn.BatchNorm2d(num_features=1)\n",
    "        self.selu = nn.SELU(inplace=True)\n",
    "\n",
    "        self.res_encoder = nn.Sequential(\n",
    "            nn.Sequential(Res2Block(nb_filts=filts[1])),\n",
    "            nn.Sequential(Res2Block(nb_filts=filts[2])),\n",
    "            nn.Sequential(Res2Block(nb_filts=filts[3])),\n",
    "            nn.Sequential(Res2Block(nb_filts=filts[4])),\n",
    "            nn.Sequential(Res2Block(nb_filts=filts[4])),\n",
    "            nn.Sequential(Res2Block(nb_filts=filts[4])))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.sinc_conv(x)\n",
    "        x = x.unsqueeze(dim=1)\n",
    "\n",
    "        x = F.max_pool2d(torch.abs(x), (3, 3))\n",
    "        x = self.first_bn(x)\n",
    "        x = self.selu(x)\n",
    "\n",
    "\n",
    "        e = self.res_encoder(x)\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05c4d5b5-b348-482e-8648-d0c3ce6068bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00dcac8f-1b46-46db-8f8e-5a731760acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51468071-f453-4c58-9dee-7ce638bfa041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.tempCNN1 = TemporalConvNet(64,[72,36,24,12,6])\n",
    "        self.tempCNN2 = TemporalConvNet(64,[72,36,24,12,6])\n",
    "        self.relu = nn.ReLU(0.1)\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.linear1 = nn.Linear(138,4)\n",
    "        self.linear2 = nn.Linear(174,4)\n",
    "        self.linear3 = nn.Linear(8,54)\n",
    "        self.linear4 = nn.Linear(54,2)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        matrix1, _ = torch.max(x, dim=2) # T\n",
    "        matrix2, _ = torch.max(x, dim=3) # S\n",
    "        x1 = self.tempCNN1(matrix2)\n",
    "        x1 = torch.flatten(x1,1,2)\n",
    "        x1 = self.linear1(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x2 = self.tempCNN2(matrix1)\n",
    "        x2 = torch.flatten(x2,1,2)\n",
    "        x2 = self.linear2(x2)\n",
    "        x2 = self.drop(x2)\n",
    "        x2 = self.relu(x2)\n",
    "\n",
    "        last_layer =self.relu(self.linear3(torch.cat((x1,x2), dim=1)))\n",
    "        return last_layer, self.linear4(last_layer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd96094d-80ac-4a4f-9466-1ae71ff91f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def genSpoof_list(dir_meta, is_train=False, is_eval=False):\n",
    "    d_meta = {}\n",
    "    file_list = []\n",
    "    with open(dir_meta, \"r\") as f:\n",
    "        l_meta = f.readlines()\n",
    "\n",
    "    if is_train:\n",
    "        for line in l_meta:\n",
    "            _, key, _, _, label = line.strip().split(\" \")\n",
    "            file_list.append(key)\n",
    "            d_meta[key] = 1 if label == \"bonafide\" else 0\n",
    "        return d_meta, file_list\n",
    "\n",
    "    elif is_eval:\n",
    "        for line in l_meta:\n",
    "            _, key, _, _, _ = line.strip().split(\" \")\n",
    "            #key = line.strip()\n",
    "            file_list.append(key)\n",
    "        return file_list\n",
    "    else:\n",
    "        for line in l_meta:\n",
    "            _, key, _, _, label = line.strip().split(\" \")\n",
    "            file_list.append(key)\n",
    "            d_meta[key] = 1 if label == \"bonafide\" else 0\n",
    "        return d_meta, file_list\n",
    "\n",
    "\n",
    "def pad(x, max_len=64600):\n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    # need to pad\n",
    "    num_repeats = int(max_len / x_len) + 1\n",
    "    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n",
    "    return padded_x\n",
    "\n",
    "\n",
    "def pad_random(x: np.ndarray, max_len: int = 64600):\n",
    "    x_len = x.shape[0]\n",
    "    # if duration is already long enough\n",
    "    if x_len >= max_len:\n",
    "        stt = np.random.randint(x_len - max_len)\n",
    "        return x[stt:stt + max_len]\n",
    "\n",
    "    # if too short\n",
    "    num_repeats = int(max_len / x_len) + 1\n",
    "    padded_x = np.tile(x, (num_repeats))[:max_len]\n",
    "    return padded_x\n",
    "\n",
    "class Dataset_ASVspoof2019_train(Dataset):\n",
    "    def __init__(self, list_IDs, labels, base_dir):\n",
    "        \"\"\"self.list_IDs\t: list of strings (each string: utt key),\n",
    "           self.labels      : dictionary (key: utt key, value: label integer)\"\"\"\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.base_dir = base_dir\n",
    "        self.cut = 64600  # take ~4 sec audio (64600 samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.list_IDs[index]\n",
    "        X, _ = sf.read(str(f\"{self.base_dir}/flac/{key}.flac\"))\n",
    "        X_pad = pad_random(X, self.cut)\n",
    "        x_inp = Tensor(X_pad)\n",
    "        y = self.labels[key]\n",
    "        return x_inp, y\n",
    "\n",
    "\n",
    "class Dataset_ASVspoof2019_devNeval(Dataset):\n",
    "    def __init__(self, list_IDs, base_dir):\n",
    "        \"\"\"self.list_IDs: list of strings (each string: utt key),\n",
    "        \"\"\"\n",
    "        self.list_IDs = list_IDs\n",
    "        self.base_dir = base_dir\n",
    "        self.cut = 64600  # take ~4 sec audio (64600 samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.list_IDs[index]\n",
    "        X, _ = sf.read(str(f\"{self.base_dir}/flac/{key}.flac\"))\n",
    "        X_pad = pad(X, self.cut)\n",
    "        x_inp = Tensor(X_pad)\n",
    "        return x_inp, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d59f368-cf81-4b83-b86e-394ca6582166",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =config['batch_size']\n",
    "epochs = config['epochs']\n",
    "\n",
    "trn_list_path = \"../LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "trn_database_path = \"../LA/ASVspoof2019_LA_train\"\n",
    "\n",
    "dev_trial_path = \"../LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n",
    "dev_database_path = \"../LA/ASVspoof2019_LA_dev\"\n",
    "\n",
    "eval_trial_path = \"../LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\"\n",
    "eval_database_path = \"../LA/ASVspoof2019_LA_eval\"\n",
    "\n",
    "asv_score_file_path = \"../LA/ASVspoof2019_LA_asv_scores/ASVspoof2019.LA.asv.eval.gi.trl.scores.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d1cdc8d-7701-4074-be92-4b50d3d92a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_label_trn, file_train = genSpoof_list(dir_meta=trn_list_path,\n",
    "                                            is_train=True,\n",
    "                                            is_eval=False)\n",
    "train_set = Dataset_ASVspoof2019_train(list_IDs=file_train, labels=d_label_trn, base_dir=trn_database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "091c8c19-c3af-439f-a6e0-58e74a5d1846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd21c434970>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = torch.Generator()\n",
    "gen.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e1f84a8-bc57-4061-84e1-601d3a2b006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = DataLoader(train_set,batch_size=batch_size,shuffle=True, drop_last=True, pin_memory=True, num_workers=4,generator=gen)\n",
    "_, file_dev = genSpoof_list(dir_meta=dev_trial_path,is_train=False,is_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6eb179e1-f447-4ab4-8e3a-95b6e7848ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = Dataset_ASVspoof2019_devNeval(list_IDs=file_dev,\n",
    "                                            base_dir=dev_database_path)\n",
    "dev_loader = DataLoader(dev_set, batch_size=batch_size,shuffle=False,drop_last=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aade071-ccd4-4a1a-8445-dcc65172cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_eval = genSpoof_list(dir_meta=eval_trial_path,\n",
    "                              is_train=False,\n",
    "                              is_eval=True)\n",
    "eval_set = Dataset_ASVspoof2019_devNeval(list_IDs=file_eval, base_dir=eval_database_path)\n",
    "\n",
    "eval_loader = DataLoader(eval_set,batch_size=batch_size, shuffle=False,drop_last=False,pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f0122f6-e809-4d77-8d44-29627aa4e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_evaluation_file(\n",
    "    data_loader: DataLoader,\n",
    "    model,\n",
    "    device: torch.device,\n",
    "    save_path: str,\n",
    "    trial_path: str) -> None:\n",
    "    \"\"\"Perform evaluation and save the score to a file\"\"\"\n",
    "    model.eval()\n",
    "    with open(trial_path, \"r\") as f_trl:\n",
    "        trial_lines = f_trl.readlines()\n",
    "    fname_list = []\n",
    "    score_list = []\n",
    "    for batch_x, utt_id in progressbar(data_loader, prefix=\"computing metrics\"):\n",
    "        batch_x = batch_x.to(device)\n",
    "        with torch.no_grad():\n",
    "            _, batch_out = model(batch_x)\n",
    "            batch_score = (batch_out[:, 1]).data.cpu().numpy().ravel()\n",
    "        # add outputs\n",
    "        fname_list.extend(utt_id)\n",
    "        score_list.extend(batch_score.tolist())\n",
    "\n",
    "    assert len(trial_lines) == len(fname_list) == len(score_list)\n",
    "    with open(save_path, \"w\") as fh:\n",
    "        for fn, sco, trl in zip(fname_list, score_list, trial_lines):\n",
    "            _, utt_id, _, src, key = trl.strip().split(' ')\n",
    "            assert fn == utt_id\n",
    "            fh.write(\"{} {} {} {}\\n\".format(utt_id, src, key, sco))\n",
    "    print(\"Scores saved to {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b971f3a2-7600-471f-b5ac-d8e932e435aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_tDCF_EER(cm_scores_file,\n",
    "                       asv_score_file,\n",
    "                       output_file,\n",
    "                       printout=True):\n",
    "    # Replace CM scores with your own scores or provide score file as the\n",
    "    # first argument.\n",
    "    # cm_scores_file =  'score_cm.txt'\n",
    "    # Replace ASV scores with organizers' scores or provide score file as\n",
    "    # the second argument.\n",
    "    # asv_score_file = 'ASVspoof2019.LA.asv.eval.gi.trl.scores.txt'\n",
    "\n",
    "    # Fix tandem detection cost function (t-DCF) parameters\n",
    "    Pspoof = 0.05\n",
    "    cost_model = {\n",
    "        'Pspoof': Pspoof,  # Prior probability of a spoofing attack\n",
    "        'Ptar': (1 - Pspoof) * 0.99,  # Prior probability of target speaker\n",
    "        'Pnon': (1 - Pspoof) * 0.01,  # Prior probability of nontarget speaker\n",
    "        'Cmiss': 1,  # Cost of ASV system falsely rejecting target speaker\n",
    "        'Cfa': 10,  # Cost of ASV system falsely accepting nontarget speaker\n",
    "        'Cmiss_asv': 1,  # Cost of ASV system falsely rejecting target speaker\n",
    "        'Cfa_asv':\n",
    "        10,  # Cost of ASV system falsely accepting nontarget speaker\n",
    "        'Cmiss_cm': 1,  # Cost of CM system falsely rejecting target speaker\n",
    "        'Cfa_cm': 10,  # Cost of CM system falsely accepting spoof\n",
    "    }\n",
    "\n",
    "    # Load organizers' ASV scores\n",
    "    asv_data = np.genfromtxt(asv_score_file, dtype=str)\n",
    "    # asv_sources = asv_data[:, 0]\n",
    "    asv_keys = asv_data[:, 1]\n",
    "    asv_scores = asv_data[:, 2].astype(np.float64)\n",
    "\n",
    "    # Load CM scores\n",
    "    cm_data = np.genfromtxt(cm_scores_file, dtype=str)\n",
    "    # cm_utt_id = cm_data[:, 0]\n",
    "    cm_sources = cm_data[:, 1]\n",
    "    cm_keys = cm_data[:, 2]\n",
    "    cm_scores = cm_data[:, 3].astype(np.float64)\n",
    "\n",
    "    # Extract target, nontarget, and spoof scores from the ASV scores\n",
    "    tar_asv = asv_scores[asv_keys == 'target']\n",
    "    non_asv = asv_scores[asv_keys == 'nontarget']\n",
    "    spoof_asv = asv_scores[asv_keys == 'spoof']\n",
    "\n",
    "    # Extract bona fide (real human) and spoof scores from the CM scores\n",
    "    bona_cm = cm_scores[cm_keys == 'bonafide']\n",
    "    spoof_cm = cm_scores[cm_keys == 'spoof']\n",
    "\n",
    "    # EERs of the standalone systems and fix ASV operating point to\n",
    "    # EER threshold\n",
    "    eer_asv, asv_threshold = compute_eer(tar_asv, non_asv)\n",
    "    eer_cm = compute_eer(bona_cm, spoof_cm)[0]\n",
    "\n",
    "    attack_types = [f'A{_id:02d}' for _id in range(7, 20)]\n",
    "    if printout:\n",
    "        spoof_cm_breakdown = {\n",
    "            attack_type: cm_scores[cm_sources == attack_type]\n",
    "            for attack_type in attack_types\n",
    "        }\n",
    "\n",
    "        eer_cm_breakdown = {\n",
    "            attack_type: compute_eer(bona_cm,\n",
    "                                     spoof_cm_breakdown[attack_type])[0]\n",
    "            for attack_type in attack_types\n",
    "        }\n",
    "\n",
    "    [Pfa_asv, Pmiss_asv,\n",
    "     Pmiss_spoof_asv] = obtain_asv_error_rates(tar_asv, non_asv, spoof_asv,\n",
    "                                               asv_threshold)\n",
    "\n",
    "    # Compute t-DCF\n",
    "    tDCF_curve, CM_thresholds = compute_tDCF(bona_cm,\n",
    "                                             spoof_cm,\n",
    "                                             Pfa_asv,\n",
    "                                             Pmiss_asv,\n",
    "                                             Pmiss_spoof_asv,\n",
    "                                             cost_model,\n",
    "                                             print_cost=False)\n",
    "\n",
    "    # Minimum t-DCF\n",
    "    min_tDCF_index = np.argmin(tDCF_curve)\n",
    "    min_tDCF = tDCF_curve[min_tDCF_index]\n",
    "\n",
    "    if printout:\n",
    "        with open(output_file, \"w\") as f_res:\n",
    "            f_res.write('\\nCM SYSTEM\\n')\n",
    "            f_res.write('\\tEER\\t\\t= {:8.9f} % '\n",
    "                        '(Equal error rate for countermeasure)\\n'.format(\n",
    "                            eer_cm * 100))\n",
    "\n",
    "            f_res.write('\\nTANDEM\\n')\n",
    "            f_res.write('\\tmin-tDCF\\t\\t= {:8.9f}\\n'.format(min_tDCF))\n",
    "\n",
    "            f_res.write('\\nBREAKDOWN CM SYSTEM\\n')\n",
    "            for attack_type in attack_types:\n",
    "                _eer = eer_cm_breakdown[attack_type] * 100\n",
    "                f_res.write(\n",
    "                    f'\\tEER {attack_type}\\t\\t= {_eer:8.9f} % (Equal error rate for {attack_type}\\n'\n",
    "                )\n",
    "        os.system(f\"cat {output_file}\")\n",
    "\n",
    "    return eer_cm * 100, min_tDCF\n",
    "\n",
    "\n",
    "def obtain_asv_error_rates(tar_asv, non_asv, spoof_asv, asv_threshold):\n",
    "\n",
    "    # False alarm and miss rates for ASV\n",
    "    Pfa_asv = sum(non_asv >= asv_threshold) / non_asv.size\n",
    "    Pmiss_asv = sum(tar_asv < asv_threshold) / tar_asv.size\n",
    "\n",
    "    # Rate of rejecting spoofs in ASV\n",
    "    if spoof_asv.size == 0:\n",
    "        Pmiss_spoof_asv = None\n",
    "    else:\n",
    "        Pmiss_spoof_asv = np.sum(spoof_asv < asv_threshold) / spoof_asv.size\n",
    "\n",
    "    return Pfa_asv, Pmiss_asv, Pmiss_spoof_asv\n",
    "\n",
    "\n",
    "def compute_det_curve(target_scores, nontarget_scores):\n",
    "\n",
    "    n_scores = target_scores.size + nontarget_scores.size\n",
    "    all_scores = np.concatenate((target_scores, nontarget_scores))\n",
    "    labels = np.concatenate(\n",
    "        (np.ones(target_scores.size), np.zeros(nontarget_scores.size)))\n",
    "\n",
    "    # Sort labels based on scores\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # Compute false rejection and false acceptance rates\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = nontarget_scores.size - \\\n",
    "        (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "\n",
    "    # false rejection rates\n",
    "    frr = np.concatenate(\n",
    "        (np.atleast_1d(0), tar_trial_sums / target_scores.size))\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums /\n",
    "                          nontarget_scores.size))  # false acceptance rates\n",
    "    # Thresholds are the sorted scores\n",
    "    thresholds = np.concatenate(\n",
    "        (np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))\n",
    "\n",
    "    return frr, far, thresholds\n",
    "\n",
    "\n",
    "def compute_eer(target_scores, nontarget_scores):\n",
    "    \"\"\" Returns equal error rate (EER) and the corresponding threshold. \"\"\"\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer, thresholds[min_index]\n",
    "\n",
    "\n",
    "def compute_tDCF(bonafide_score_cm, spoof_score_cm, Pfa_asv, Pmiss_asv,\n",
    "                 Pmiss_spoof_asv, cost_model, print_cost):\n",
    "    \"\"\"\n",
    "    Compute Tandem Detection Cost Function (t-DCF) [1] for a fixed ASV system.\n",
    "    In brief, t-DCF returns a detection cost of a cascaded system of this form,\n",
    "\n",
    "      Speech waveform -> [CM] -> [ASV] -> decision\n",
    "\n",
    "    where CM stands for countermeasure and ASV for automatic speaker\n",
    "    verification. The CM is therefore used as a 'gate' to decided whether or\n",
    "    not the input speech sample should be passed onwards to the ASV system.\n",
    "    Generally, both CM and ASV can do detection errors. Not all those errors\n",
    "    are necessarily equally cost, and not all types of users are necessarily\n",
    "    equally likely. The tandem t-DCF gives a principled with to compare\n",
    "    different spoofing countermeasures under a detection cost function\n",
    "    framework that takes that information into account.\n",
    "\n",
    "    INPUTS:\n",
    "\n",
    "      bonafide_score_cm   A vector of POSITIVE CLASS (bona fide or human)\n",
    "                          detection scores obtained by executing a spoofing\n",
    "                          countermeasure (CM) on some positive evaluation trials.\n",
    "                          trial represents a bona fide case.\n",
    "      spoof_score_cm      A vector of NEGATIVE CLASS (spoofing attack)\n",
    "                          detection scores obtained by executing a spoofing\n",
    "                          CM on some negative evaluation trials.\n",
    "      Pfa_asv             False alarm (false acceptance) rate of the ASV\n",
    "                          system that is evaluated in tandem with the CM.\n",
    "                          Assumed to be in fractions, not percentages.\n",
    "      Pmiss_asv           Miss (false rejection) rate of the ASV system that\n",
    "                          is evaluated in tandem with the spoofing CM.\n",
    "                          Assumed to be in fractions, not percentages.\n",
    "      Pmiss_spoof_asv     Miss rate of spoof samples of the ASV system that\n",
    "                          is evaluated in tandem with the spoofing CM. That\n",
    "                          is, the fraction of spoof samples that were\n",
    "                          rejected by the ASV system.\n",
    "      cost_model          A struct that contains the parameters of t-DCF,\n",
    "                          with the following fields.\n",
    "\n",
    "                          Ptar        Prior probability of target speaker.\n",
    "                          Pnon        Prior probability of nontarget speaker (zero-effort impostor)\n",
    "                          Psoof       Prior probability of spoofing attack.\n",
    "                          Cmiss_asv   Cost of ASV falsely rejecting target.\n",
    "                          Cfa_asv     Cost of ASV falsely accepting nontarget.\n",
    "                          Cmiss_cm    Cost of CM falsely rejecting target.\n",
    "                          Cfa_cm      Cost of CM falsely accepting spoof.\n",
    "\n",
    "      print_cost          Print a summary of the cost parameters and the\n",
    "                          implied t-DCF cost function?\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "      tDCF_norm           Normalized t-DCF curve across the different CM\n",
    "                          system operating points; see [2] for more details.\n",
    "                          Normalized t-DCF > 1 indicates a useless\n",
    "                          countermeasure (as the tandem system would do\n",
    "                          better without it). min(tDCF_norm) will be the\n",
    "                          minimum t-DCF used in ASVspoof 2019 [2].\n",
    "      CM_thresholds       Vector of same size as tDCF_norm corresponding to\n",
    "                          the CM threshold (operating point).\n",
    "\n",
    "    NOTE:\n",
    "    o     In relative terms, higher detection scores values are assumed to\n",
    "          indicate stronger support for the bona fide hypothesis.\n",
    "    o     You should provide real-valued soft scores, NOT hard decisions. The\n",
    "          recommendation is that the scores are log-likelihood ratios (LLRs)\n",
    "          from a bonafide-vs-spoof hypothesis based on some statistical model.\n",
    "          This, however, is NOT required. The scores can have arbitrary range\n",
    "          and scaling.\n",
    "    o     Pfa_asv, Pmiss_asv, Pmiss_spoof_asv are in fractions, not percentages.\n",
    "\n",
    "    References:\n",
    "\n",
    "      [1] T. Kinnunen, K.-A. Lee, H. Delgado, N. Evans, M. Todisco,\n",
    "          M. Sahidullah, J. Yamagishi, D.A. Reynolds: \"t-DCF: a Detection\n",
    "          Cost Function for the Tandem Assessment of Spoofing Countermeasures\n",
    "          and Automatic Speaker Verification\", Proc. Odyssey 2018: the\n",
    "          Speaker and Language Recognition Workshop, pp. 312--319, Les Sables d'Olonne,\n",
    "          France, June 2018 (https://www.isca-speech.org/archive/Odyssey_2018/pdfs/68.pdf)\n",
    "\n",
    "      [2] ASVspoof 2019 challenge evaluation plan\n",
    "          TODO: <add link>\n",
    "    \"\"\"\n",
    "\n",
    "    # Sanity check of cost parameters\n",
    "    if cost_model['Cfa_asv'] < 0 or cost_model['Cmiss_asv'] < 0 or \\\n",
    "            cost_model['Cfa_cm'] < 0 or cost_model['Cmiss_cm'] < 0:\n",
    "        print('WARNING: Usually the cost values should be positive!')\n",
    "\n",
    "    if cost_model['Ptar'] < 0 or cost_model['Pnon'] < 0 or cost_model['Pspoof'] < 0 or \\\n",
    "            np.abs(cost_model['Ptar'] + cost_model['Pnon'] + cost_model['Pspoof'] - 1) > 1e-10:\n",
    "        sys.exit(\n",
    "            'ERROR: Your prior probabilities should be positive and sum up to one.'\n",
    "        )\n",
    "\n",
    "    # Unless we evaluate worst-case model, we need to have some spoof tests against asv\n",
    "    if Pmiss_spoof_asv is None:\n",
    "        sys.exit(\n",
    "            'ERROR: you should provide miss rate of spoof tests against your ASV system.'\n",
    "        )\n",
    "\n",
    "    # Sanity check of scores\n",
    "    combined_scores = np.concatenate((bonafide_score_cm, spoof_score_cm))\n",
    "    if np.isnan(combined_scores).any() or np.isinf(combined_scores).any():\n",
    "        sys.exit('ERROR: Your scores contain nan or inf.')\n",
    "\n",
    "    # Sanity check that inputs are scores and not decisions\n",
    "    n_uniq = np.unique(combined_scores).size\n",
    "    if n_uniq < 3:\n",
    "        sys.exit(\n",
    "            'ERROR: You should provide soft CM scores - not binary decisions')\n",
    "\n",
    "    # Obtain miss and false alarm rates of CM\n",
    "    Pmiss_cm, Pfa_cm, CM_thresholds = compute_det_curve(\n",
    "        bonafide_score_cm, spoof_score_cm)\n",
    "\n",
    "    # Constants - see ASVspoof 2019 evaluation plan\n",
    "    C1 = cost_model['Ptar'] * (cost_model['Cmiss_cm'] - cost_model['Cmiss_asv'] * Pmiss_asv) - \\\n",
    "        cost_model['Pnon'] * cost_model['Cfa_asv'] * Pfa_asv\n",
    "    C2 = cost_model['Cfa_cm'] * cost_model['Pspoof'] * (1 - Pmiss_spoof_asv)\n",
    "\n",
    "    # Sanity check of the weights\n",
    "    if C1 < 0 or C2 < 0:\n",
    "        sys.exit(\n",
    "            'You should never see this error but I cannot evalute tDCF with negative weights - please check whether your ASV error rates are correctly computed?'\n",
    "        )\n",
    "\n",
    "    # Obtain t-DCF curve for all thresholds\n",
    "    tDCF = C1 * Pmiss_cm + C2 * Pfa_cm\n",
    "\n",
    "    # Normalized t-DCF\n",
    "    tDCF_norm = tDCF / np.minimum(C1, C2)\n",
    "\n",
    "    # Everything should be fine if reaching here.\n",
    "    if print_cost:\n",
    "\n",
    "        print('t-DCF evaluation from [Nbona={}, Nspoof={}] trials\\n'.format(\n",
    "            bonafide_score_cm.size, spoof_score_cm.size))\n",
    "        print('t-DCF MODEL')\n",
    "        print('   Ptar         = {:8.5f} (Prior probability of target user)'.\n",
    "              format(cost_model['Ptar']))\n",
    "        print(\n",
    "            '   Pnon         = {:8.5f} (Prior probability of nontarget user)'.\n",
    "            format(cost_model['Pnon']))\n",
    "        print(\n",
    "            '   Pspoof       = {:8.5f} (Prior probability of spoofing attack)'.\n",
    "            format(cost_model['Pspoof']))\n",
    "        print(\n",
    "            '   Cfa_asv      = {:8.5f} (Cost of ASV falsely accepting a nontarget)'\n",
    "            .format(cost_model['Cfa_asv']))\n",
    "        print(\n",
    "            '   Cmiss_asv    = {:8.5f} (Cost of ASV falsely rejecting target speaker)'\n",
    "            .format(cost_model['Cmiss_asv']))\n",
    "        print(\n",
    "            '   Cfa_cm       = {:8.5f} (Cost of CM falsely passing a spoof to ASV system)'\n",
    "            .format(cost_model['Cfa_cm']))\n",
    "        print(\n",
    "            '   Cmiss_cm     = {:8.5f} (Cost of CM falsely blocking target utterance which never reaches ASV)'\n",
    "            .format(cost_model['Cmiss_cm']))\n",
    "        print(\n",
    "            '\\n   Implied normalized t-DCF function (depends on t-DCF parameters and ASV errors), s=CM threshold)'\n",
    "        )\n",
    "\n",
    "        if C2 == np.minimum(C1, C2):\n",
    "            print(\n",
    "                '   tDCF_norm(s) = {:8.5f} x Pmiss_cm(s) + Pfa_cm(s)\\n'.format(\n",
    "                    C1 / C2))\n",
    "        else:\n",
    "            print(\n",
    "                '   tDCF_norm(s) = Pmiss_cm(s) + {:8.5f} x Pfa_cm(s)\\n'.format(\n",
    "                    C2 / C1))\n",
    "\n",
    "    return tDCF_norm, CM_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f53ff704-47e6-4fe8-b03a-8e16ceecb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(trn_loader, model, optim, device):\n",
    "    running_loss = 0\n",
    "    num_total = 0.0\n",
    "    ii = 0\n",
    "    model.train()\n",
    "    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    for batch_x, batch_y in progressbar(trn_loader):\n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "        ii += 1\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
    "        _, batch_out = model(batch_x)\n",
    "        batch_loss = criterion(batch_out, batch_y)\n",
    "        running_loss += batch_loss.item() * batch_size\n",
    "        optim.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optim.step()\n",
    "    running_loss /= num_total\n",
    "    return running_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84cb45cd-71a7-4715-8445-8733b4ed65c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "best_dev_eer = 2.\n",
    "best_eval_eer = 2.\n",
    "best_dev_tdcf = 0.05\n",
    "best_eval_tdcf = 1.\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available else \"cpu\"\n",
    "model = TestModel().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b0e49-2adb-4cef-af87-977d55e90691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[â–ˆâ–ˆ..........................................................] 35/906 time 00:45.20 / 18:44.78\r"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = train_epoch(trn_loader, model, optimizer, device)\n",
    "    produce_evaluation_file(dev_loader, model, device, \"dev_score.txt\", dev_trial_path)\n",
    "    dev_eer, dev_tdcf = calculate_tDCF_EER(\n",
    "            cm_scores_file=\"dev_score.txt\",\n",
    "            asv_score_file=asv_score_file_path,\n",
    "            output_file=\"dev_t-DCF_EER_{}epo.txt\".format(epoch),\n",
    "            printout=False)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_loss\": running_loss,\n",
    "        \"dev_eer\": dev_eer,\n",
    "        \"dev_tdcf\": dev_tdcf\n",
    "    }\n",
    "    wandb.log(metrics)\n",
    "    \n",
    "    best_dev_tdcf = min(dev_tdcf, best_dev_tdcf)\n",
    "    if best_dev_eer >= dev_eer:\n",
    "        best_dev_eer = dev_eer\n",
    "        torch.save(model.state_dict(), \"epoch_{}_{:03.3f}.pth\".format(epoch, dev_eer))\n",
    "\n",
    "        produce_evaluation_file(eval_loader, model, device, \"eval_score.txt\", eval_trial_path)\n",
    "        eval_eer, eval_tdcf = calculate_tDCF_EER(\n",
    "                    cm_scores_file=\"eval_score.txt\",\n",
    "                    asv_score_file=asv_score_file_path,\n",
    "                    output_file=\"eval_t-DCF_EER_{}epo.txt\".format(epoch),printout=False)\n",
    "        metrics = {\n",
    "            \"eval_eer\": eval_eer,\n",
    "            \"eval_tdcf\": eval_tdcf\n",
    "        }\n",
    "        wandb.log(metrics)\n",
    "        if best_eval_eer >= eval_eer:\n",
    "            best_eval_eer = eval_eer\n",
    "            torch.save(model.state_dict(), \"best_{:03.3f}.pth\".format(eval_eer))\n",
    "            \n",
    "            \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f91ef0-41d9-4e2c-b46a-8fab22ee0828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
